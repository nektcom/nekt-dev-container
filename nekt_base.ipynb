{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Data Access Token\n",
    "\n",
    "You have to generate an access token in order to securely access data from your Lake at Nekt. When generating the token you can associate it with one or more tables.\n",
    "\n",
    "Follow the steps on this link to get your token:\n",
    "https://app.nekt.ai/transformations/resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "resp = requests.get(\n",
    "    \"https://api.nekt.ai/api/v1/jupyter-credentials/\",\n",
    "    headers={\"X-Jupyter-Token\": \"<<INSERT_DATA_ACCESS_TOKEN_HERE>>\"},\n",
    ")\n",
    "credentials = resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark session\n",
    "\n",
    "This code block starts a Spark session for your data transformation. No need to change it, unless you know what you're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session with your AWS Credentials - DO NOT CHANGE THIS!\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .setAppName(\"Nekt-Transformation\")  # replace with your desired name\n",
    "    .set(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0,org.apache.hadoop:hadoop-aws:3.3.4\")\n",
    "    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", credentials[\"aws_access_key_id\"])\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", credentials[\"aws_secret_access_key\"])\n",
    "    .set(\"spark.hadoop.fs.s3a.session.token\", credentials[\"aws_session_token\"])  # optional\n",
    "    .set(\"spark.sql.shuffle.partitions\", \"4\")  # default is 200 partitions which is too many for local\n",
    "    .setMaster(\"local[*]\")  # replace the * with your desired number of cores. * for use all.\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tables from your Nekt catalog\n",
    "\n",
    "Replace the `INPUT_TABLES` variable with the list of input tables associated with your token. \n",
    "\n",
    "You can access the list by clicking on 'Token tables' on the previosuly created token (https://app.nekt.ai/transformations/resources)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TABLES = [] # REPLACE THIS WITH THE LIST OF INPUT_TABLES \n",
    "\n",
    "### Load input tables - DO NOT CHANGE THIS!\n",
    "delta_dict = {}\n",
    "for table in INPUT_TABLES:\n",
    "    table_layer = table.get(\"layer\")\n",
    "    table_name = table.get(\"name\")\n",
    "    if not delta_dict.get(table_layer):\n",
    "        delta_dict[table_layer] = {}\n",
    "\n",
    "    try:\n",
    "        delta_dict[table_layer][table_name] = DeltaTable.forPath(spark, table.get(\"path\"))\n",
    "    except:\n",
    "        delta_dict[table_layer][table_name] = None\n",
    "        print(f'Failed to load delta table \"{table_name}\" from layer \"{table_layer}\".')\n",
    "    else:\n",
    "        print(f'Delta table \"{table_name}\" loaded from layer \"{table_layer}\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframes\n",
    "\n",
    "For each data frame you want to access in your notebook, run the following code:\n",
    "\n",
    "```\n",
    "df: DataFrame = delta_dict.get(\"layer_name\").get(\"table_name\").toDF()\n",
    "```\n",
    "\n",
    "- `layer_name` is a string with the name of the layer where your table is located, for example: 'service' or 'trusted'\n",
    "- `table_name` is a string with the name of the table you want to access\n",
    "\n",
    "The table names and layers are all listed in the `INPUT_TABLES` variable from the **Load input tables** section above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df: DataFrame = delta_dict.get(\"INSERT_LAYER_NAME_HERE\").get(\"INSERT_TABLE_NAME_HERE\").toDF() # rename your data frame as you wish!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation script\n",
    "\n",
    "Add your transformation code here, feel free to create multiple sections for exploration, test and validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final transformation function\n",
    "\n",
    "Once your script is validated, you have to put it all together in a `user_transformation` function. This function receives a list of tables as input and generates an output table.\n",
    "\n",
    "Avoid using commands to print something out, count or do any verification such as `.show()`, `.count()` and others, since they will slow down your data transformation and consume more resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_transformation(delta_dict):\n",
    "    \n",
    "    # Load dataframes\n",
    "    df: DataFrame = delta_dict.get(\"INSERT_LAYER_NAME_HERE\").get(\"INSERT_TABLE_NAME_HERE\").toDF()\n",
    "    \n",
    "    # Transformation script\n",
    "    # ...\n",
    "    # ...\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation test\n",
    "\n",
    "Run this block of code to ensure you haven't missed andy piece of code and your final function is returning the desired dataframe.\n",
    "\n",
    "You DON'T need to copy it to the platform, it's just a verification step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = user_transformation(delta_dict)\n",
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add transformation to Nekt\n",
    "\n",
    "Now that you have tested your final function, go to [Add transformation](https://app.nekt.ai/transformations/add-transformation) at Nekt, select your input tables, give your new table a name, and paste the `user_transformation(delta_dict)` in the code section. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
