{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before you start, you should read the **README** file carefully. \n",
    "# There, we walk you through setting up your local environment - a crucial step to ensure proper execution of the code in the sections below.\n",
    "# Happy coding! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert Access Token to get access to AWS environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "resp = requests.get(\n",
    "    \"https://api.nekt.ai/api/v1/jupyter-credentials/\",\n",
    "    headers={\"X-Jupyter-Token\": \"INSERT_ACCESS_TOKEN_HERE\"},\n",
    ")\n",
    "credentials = resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session with your AWS Credentials\n",
    "# Check comments in the lines for further instruction\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .setAppName(\"Nekt-Transformation\")  # replace with your desired name\n",
    "    .set(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0,org.apache.hadoop:hadoop-aws:3.3.4\")\n",
    "    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", credentials[\"aws_access_key_id\"])\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", credentials[\"aws_secret_access_key\"])\n",
    "    .set(\"spark.hadoop.fs.s3a.session.token\", credentials[\"aws_session_token\"])  # optional\n",
    "    .set(\"spark.sql.shuffle.partitions\", \"4\")  # default is 200 partitions which is too many for local\n",
    "    .setMaster(\"local[*]\")  # replace the * with your desired number of cores. * for use all.\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load input tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TABLES = []  # replace with your input tables code generated on the platform\n",
    "\n",
    "### Load input tables\n",
    "delta_dict = {}\n",
    "for table in INPUT_TABLES:\n",
    "    table_layer = table.get(\"layer\")\n",
    "    table_name = table.get(\"name\")\n",
    "    if not delta_dict.get(table_layer):\n",
    "        delta_dict[table_layer] = {}\n",
    "\n",
    "    try:\n",
    "        delta_dict[table_layer][table_name] = DeltaTable.forPath(spark, table.get(\"path\"))\n",
    "    except:\n",
    "        delta_dict[table_layer][table_name] = None\n",
    "        print(f'Failed to load delta table \"{table_name}\" from layer \"{table_layer}\".')\n",
    "    else:\n",
    "        print(f'Delta table \"{table_name}\" loaded from layer \"{table_layer}\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [OPTIONAL] Add more dependencies here as needed to perform your transformation\n",
    "# Since we're using poetry, you should run `poetry add <package-name>` in order to install it in your virtual environment\n",
    "# A list of existing dependencies can be found on the poetry.lock file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each data frame you want to access in your notebook, run the following code:\n",
    "\n",
    "# delta_table: DeltaTable = delta_dict.get(\"layer_name\").get(\"table_name\")\n",
    "# df: DataFrame = delta_table.toDF()\n",
    "\n",
    "# Where `layer_name` is a string with the name of the layer that contains the specific table you want. For example: 'service' or 'trusted'\n",
    "# And `table_name` is a string with the name of the table you want to access\n",
    "# The table names and layers are all listed in the INPUT_TABLES object from the 'Load input tables' section above\n",
    "\n",
    "delta_table: DeltaTable = delta_dict.get(\"INSERT_LAYER_NAME_HERE\").get(\"INSERT_TABLE_NAME_HERE\")\n",
    "df: DataFrame = delta_table.toDF() # rename your data frame as you wish!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [OPTIONAL] Add custom functions to be used later in your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this section (and feel free to add more sections & sub-sections!) to run your own code related "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run some (or all) of the commands below to validate your final dataframe and make sure it is what you need\n",
    "#In these examples, 'new_df' is the final dataframe name\n",
    "\n",
    "print(new_df.columns)\n",
    "new_df.show(5, truncate=False)\n",
    "new_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, you should generate a function pasting on its body all the necessary code to get to your final dataframe. \n",
    "# You shouldn't copy to this final function commands that print something out, count or do any verification. Just the ones that are indeed part of the transformation. \n",
    "# Don't forget to test it in the next block of code!\n",
    "\n",
    "def user_transformation(delta_dict):\n",
    "    #Load dataframe section\n",
    "    delta_table: DeltaTable = delta_dict.get(\"INSERT_LAYER_NAME_HERE\").get(\"INSERT_TABLE_NAME_HERE\")\n",
    "    df: DataFrame = delta_table.toDF()\n",
    "    \n",
    "    #Custom imports section\n",
    "    \n",
    "    #Helpers section\n",
    "\n",
    "    #Transformation scripts section\n",
    "\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this block of code to ensure you haven't missed andy piece of code and your final function is returning the desired dataframe.\n",
    "#You DON'T need to copy it to the platform, it's just a verification step.\n",
    "\n",
    "dataframe = user_transformation(delta_dict)\n",
    "dataframe.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add your transformation\n",
    "\n",
    "Now that you have tested your final function, go to [Add transformation](https://app.nekt.ai/transformations/add-transformation), select your input tables, give your new table a name, and paste the `user_transformation(delta_dict)` in the code section. \n",
    "\n",
    "If you have new necessary dependencies, make sure to add them too in the 'Define your dependencies' section. Proceed with the flow and you'll have the new table in your Lakehouse ready to be used!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
